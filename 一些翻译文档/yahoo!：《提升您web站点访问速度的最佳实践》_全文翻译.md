# Yahoo!：《提升您Web站点访问速度的最佳实践》 全文翻译
---
原文（En）：[Best Practices for Speeding Up Your Web Site](https://developer.yahoo.com/performance/rules.html)

写在前面：最近一段时间是实习内推面试的高发期，个人意向岗位是Web前端开发，而这个Web开发的领域，无论是PC/移动端的前端还是后台都会非常重视性能优化，面试的问题也会频繁涉及到（PS. 个人在鹅厂WXG和SNG每一面都有，被问怕了）。

据Google统计数据，假如它的搜索页面加载速度增加500ms（相当于20%的速度降低），会导致5%~9%的流量损失（即用户在页面完全加载前就把它关闭了）。另外，网站打开的速度，在Google本身的搜索结果排名里面也会占据很大的权重，加载用时大于1.5s的网站，会被认为是比较慢的，从而导致被搜索引擎分页到第二页甚至更后面，这对网站流量的影响简直是难以想象的。

所以，这就是我翻译此文的动机所在，顺便也可以加强一下自己的专业英文水平，后面可能会和大家分享一些面试中出现频率较高的面试题，并附上我个人的回答。囿于本人水平，如有疏漏不足还请各位斧正。



正文：
---
The Exceptional Performance team has identified a number of best practices for making web pages fast. The list includes 35 best practices divided into 7 categories.

（雅虎的）卓越性能团队指出了许多可以让您的网页加载速度更快的最佳实践规则。这个列表包含了35条规则，被划分为7类。

* 注1：原文没有分类，这里我将整理一下并按标签划分，风味更佳。
* 注2：由于本文写于数年前，文中有不少地方与现在的实际情况有所出入，我会加以注明。

## 分类标签：内容

### 减少HTTP请求

终端用户的等待时间的80%都花费在后端的处理，而这部分时间的大部分都在进行页面上所有元素的加载：图片、样式、脚本、Flash等等。反过来说，减少这些元素的数量就能减少为了渲染这个页面所需的HTTP请求次数。这是加快速度的关键。

减少元素的其中一个方法是简化页面设计，但是否可以兼得丰富的内容和快速地加载呢？下面是一些减少HTTP请求次数的方法。

* **合并文件**是一个减少请求次数的方法，也就是将多个JS/CSS合并为单一的脚本或样式表文件。（译者注：移动端HTTP连接建立时间一般在百毫秒级，2G网络就更不用说了……）

* **CSS Sprites（CSS雪碧图）**是一个减少图片请求次数的好方法，合并你的多张图片成为一个单独文件，并通过CSS中`backgroung-image`和`background-position`属性控制显示的范围。（译者注：特别是在使用了大量icon和Logo的场景下，这招很管用）

* **Image map**合并多张图片为一张，然后通过坐标控制跳转。由于绘制这样的map很单调且易出错，不推荐使用。具体可以参考W3School [HTML &lt;map&gt; tag](http://www.w3schools.com/tags/tag_map.asp)。

* **内联图片**使用像`Base64`这样的编码来把图片以文本形式嵌入到HTML/CSS，但这会增加HTML/CSS文件的大小。（译者注：这个方法的复用性差，而下面提到的Expires等头部字段实现图片缓存更有利于内容的复用）

总结：减少HTTP请求次数只是开始，这也是优化初次访问性能最重要的一步。每天可能有40%~60%的用户是在完全没有本地缓存的情况下访问你的站点，因此给予他们更快的访问速度是提升用户体验的关键。

### 避免重定向

重定向一般都使用HTTP中的301和302状态码来实现。下面是一个状态码301的HTTP响应报文首部：

    HTTP/1.1 301 Moved Permanently
    Location: http://example.com/newuri
    Content-Type: text/html

这时，浏览器会自动请求`Location`字段中的url。所有重定向所必须的信息都储存在报文首部，主体部分没有任何内容。无论是301还是302响应都不会被缓存，除非它们被加上了像`Expires`或`Cache-Control`的额外字段的说明。除此之外，还可以使用`<meta>`标签的`refresh`或者JS来做到。但建议使用3XX状态码实现重定向，因为它们可以保证浏览器能正确执行后退功能。

但需要记住的是重定向会影响用户体验，特别是当重定向的过程中页面会一片空白直到从新的URL处获得HTML为止。

不必要的重定向之一，会在URL末尾应该存在`/`但缺少的这种情况下出现。例如，直接访问[http://astrology.yahoo.com/astrology](http://astrology.yahoo.com/astrology)会导致301重定向到[http://astrology.yahoo.com/astrology/](http://astrology.yahoo.com/astrology/)（**注意那个末尾增加的'/'**）一般可以通过调整服务器对请求URL的路由解析规则来解决。

另一个常见的情景是使用重定向建立一个旧网站到新网站的链接，因为这只需要简单的工作，但它往往会降低用户的体验。如果这几个不同的域名都对应同一个服务器，那么可以使用`Alias`和`mod_rewrite`这样的功能（Apache服务器）。

最后，还可以通过新建`CNAME`（把一个域名映射到另一个的DNS记录）来优化重定向。

### 减少DNS请求次数

DNS（Domain Name System，域名系统）把域名映射成IP地址。当你在地址栏输入某个网址，浏览器就会在后台发出DNS查询请求，这需要20~120ms的时间。在完成查询之前，浏览器什么都做不了。

DNS查询可以通过缓存来获得更好的性能，这会出现在特别的缓存服务器上，比如用户的ISP提供商或者地区网络节点，但它也会出现在个人的电脑上的系统DNS缓存中（windows: "DNS" client service）。

以IE为例，它会缓存用户的DNS查询结果30min，FF则是60min。因此减少域名数量对于减少响应时间有一定的作用，但是又会因为减少了并发连接数量（原因见[CDN 先留个坑](#)）从而增加时间。建议是使用2到4个独立域名比较好。


### 缓存Ajax

Ajax的其中一个好处是可以异步处理从前端到后台的请求而不需要刷新页面。例如，用户使用一个基于Web的邮件客户端时，需要不停地等待异步请求邮件信息的结果。显然，这里的优化也很有必要。

对Ajax的优化可以参考下面这些守则：

* [增加Expires或Cache-Control报文首部]()
* [使用Gzip]()
* [减少DNS查询]()
* [精简JS]()
* [避免重定向]()
* [配置Etags]()

让我们回到上面的例子，例如一个邮件客户端会使用Ajax请求通讯录用于自动完成功能。如果从上次使用到现在，用户的通讯录没有更新，那么就应该可以从缓存中复用之前的结果。这里应该控制浏览器何时可以复用何时应该发起新请求，比如在Ajax URL上面增加一个像`&t=1459144096744`的时间戳。如果在访问之后没有更新，时间戳保持不变；否则生成新的时间戳参数保证能够请求新的内容。

### 延迟加载文件

显而易见的，在加载页面的时候，某些部分需要被优先加载用以初始化页面，而另一些则可以延后。

把JS按`onload`事件前后划分是一个好的做法。比如用于拖拽和动画的库可以延迟加载，因为只有页面上DOM初始化之后才能用上它们；而隐藏内容（比如像用户操作之后出现）或图像等也是同样的道理。

能帮助你做这些的工具：

* [YUI Image Loader](https://developer.yahoo.com/yui/imageloader/)：延迟图像加载
* [YUI GET utility](https://developer.yahoo.com/yui/get/)：非阻塞加载JS/CSS

可以通过使用Chrome的network面板或FF的Firebug的net面板查看[Yahoo!首页](http://www.yahoo.com/)来更进一步理解这部分内容。

### 预加载文件

预加载看起来像延迟加载的对立面，但它们有不同的目标。这意味着能充分利用浏览器发出请求之后等待的空闲时间。

这是几种类型预加载：

* 无条件的预加载：当页面加载的时候就会被请求的文件。以Google.com为例，它会在加载的时候请求一个雪碧图（CSS Sprite），但它并不在该页面中被使用，而是在它的搜索结果页面中才会被使用。
* 有条件的预加载：基于用户的行为预加载资源。以[search.yahoo.com](http://search.yahoo.com/)为例，可以观察到当用户在输入框内输入某些内容的时候一些额外的文件就会被加载到本地。
* 有预期的加载：在更新重构的页面上线之前预加载。比如当页面需要大改之前，可利用用户访问旧站点的空闲，以提前把新资源缓存到用户本地，以减少之后新页面的加载时间。

### 减少DOM数量

复杂的页面意味着需要更久地加载时间和更慢的DOM操作。如果DOM数量太多，这意味着HTML页面中除了删除内容之外还存在可以优化的地方。比如是否使用了嵌套`table`来布局，或是引入空的`div`来修复样式等等。

[YUI CSS utilities](https://developer.yahoo.com/yui/)则可以帮你解决部分这样的问题。其中grids能够做到布局，而font和reset可以重置浏览器默认样式。另外就是更语义化的使用标签，比如不要使用`div`来引入一个新行。

通过console中输入

	document.getElementsByTagName('*').length

可以轻松得知页面中的DOM数量。多少DOM才算是多呢？可以参考制作精良的页面，比如Yahoo的首页有着众多的内容，但HTML标签的数量不超过700。

### 按域名划分页面内容

把页面划分可以实现最大化的并行加载。而这个域名的数量最好在2-4个因为DNS查询有一定代价。比如可以把HTML放在`www.example.org`而其他静态部分放在`static1.example.org`和`static2.example.org`。

更多细节请参考"[Maximizing Parallel Downloads in the Carpool Lane](http://yuiblog.com/blog/2007/04/11/performance-research-part-4/)"一文的内容。

### 减少iframe的数量

`iframe`标签允许父页面中嵌入子页面。想要高效使用的前提是明白`iframe`是如何工作的。

ifram的优点：

* 用于加载缓慢的第三方内容，比如各种广告
* 安全的沙盒（译者注：特别是HTML5新增的`sandbox`属性。关于iframe的详细说明可以参见[这里](https://segmentfault.com/a/1190000004502619)）
* 并行传输脚本等文件。

iframe的缺点：

* 就算是空白的页面也会消耗大量性能
* 阻塞父页面`onload`事件
* 非语义化的（译者注：其实非语义化就意味着给爬虫解析带来困难，或者说对搜索引擎不友好）

### 不要出现404

发起HTTP请求开销是很高的（指花费时间）而且如果得到无用的响应（比如404 Not Found）就会明显的降低用户体验。

一些网站会使用帮助性的404，比如“您要找的是不是XXX”，这会带来良好的用户体验但可能使用额外的服务器资源。最坏的情况是引用的外部JS出现404，这不仅会破坏并行加载，而且还会让浏览器尝试解析这个报文并找出其中可用的部分。这些都会影响性能。




## 分类标签：服务器

### 使用CDN（内容分发网络）

用户与服务器的距离会对响应时间产生影响，把内容分布到大量的、各地的服务器上会提升加载速度。

另外，HTTP1.1协议中规定浏览器对每个域名的并行连接数量不能超过两个（译者注：在之后的版本中废除了，但实际上现代浏览器依然存在这个限制，比如：Chrome最大并行限制是6个）。因此，把内容分散到多个拥有不同域名的服务器是一个增加并发连接数减少响应时间的好主意。

重复一遍，加载时间的绝大部分都耗费在各种页面元素上，因此在重构之前，要做的第一件事就是把静态文件发布到CDN。许多的大公司都有独立的CDN，但也存在像`Akamai Technologies`、`EdgeCast`、`level3`这样的CDN服务提供商。以雅虎为例，当它把静态文件托管给第三方CDN以及私有CDN之后，访问时间减少了约20%。

### 使用Expires或Cache-Control的报文头部字段

这规则有两个部分：

* 对于静态内容，应该把`Expires`字段设置为`Never Expire`或一个很长的时间。
* 对于动态内容，应该使用合适的`Cache-Control`字段来帮助浏览器决定是否发送请求。

由于网页内容越来越多，在初次访问之后就应该缓存部分元素，节省请求次数。`Expires`字段一般用于静态文件，比如Logo、某些框架文件或Flash播放器等元素。

浏览器（以及代理服务器）有缓存的机制，`Expires`字段可以告诉它们这个文件能被缓存的时间，比如：

	Expires: Thu, 15 Apr 2010 20:00:00 GMT

这相当于在2010年4月15日之前，浏览器都不会再去向服务器请求这一个文件。

但需要注意的是，如果使用到了这样一个字段并设置了很长的过期时间，那么在文件内容变化的时候，需要同时改变它的名字。在雅虎，文件名后一般会添加一个版本号，比如`yahoo_2.0.6.js`。

还有一点就是，使用这些字段不会减少初次访问者的HTTP请求次数。因此这个方法能带来多少性能提升取决于这个“主缓存”（主缓存包含了页面中所有元素）的命中率。在雅虎，这个命中率在75%~85%左右。

### Gzip

显然，传输HTTP请求和响应的时间取决于它们的大小，因此压缩报文大小是减少传输时间的一个好方法。从HTTP1.1开始，客户端开始支持发送带有`Accept-Encoding`首部字段的请求报文，比如：

	Accept-Encoding: gzip, deflate

如果服务器支持，那么它将会采用该字段中的某一种方法压缩响应报文，然后同样在报文首部给出相应的信息，比如：

	Content-Encoding: gzip

Gzip[[RFC 1952](http://www.ietf.org/rfc/rfc1952.txt)]是由GNU开发的压缩算法。而另一个算法deflate[[RFC 1951](http://www.ietf.org/rfc/rfc1951.txt)]（该字段仅有这两个选项）的效率和支持程度都比不上Gzip。

通常来说，Gzip的文本文件压缩率（压缩文件/原文件）都在70%左右，主流浏览器均支持。服务器一般可以自定义压缩哪些类型的文件，比如`.html/.js/.css`。除此之外，`XML`和`JSON`也都应该被压缩（译者注：这两者常被忽略）。另外，图像和PDF不应该被压缩，因为它们本身就被压缩过了，这对于效率的提升帮助不大。

### 配置Etags

实体标签（Entity tags，Etags）是服务器和浏览器用来判断某个文件是否已经被缓存的机制。（译者注：实体，也可以称为组件，一般指图像、脚本、样式表等文件）。Etags能唯一标识出某个特定文件的版本信息，唯一限制是它必须被包括在一对双引号中。服务器会在某个请求的响应报文首部说明`Etags`字段。比如：

      HTTP/1.1 200 OK
      Last-Modified: Tue, 12 Dec 2006 03:03:59 GMT
      ETag: "10c24bc-4ab-457e1c1f"
      Content-Length: 12195

假设在此之后服务器需要验证这个文件版本信息，它会使用`If-None-match`字段传输Etags信息到服务器。如果通过验证，这说明文件没有被更新，一个304状态码将会被返回给浏览器：

      GET /i/yahoo.gif HTTP/1.1
      Host: us.yimg.com
      If-Modified-Since: Tue, 12 Dec 2006 03:03:59 GMT
      If-None-Match: "10c24bc-4ab-457e1c1f"
      HTTP/1.1 304 Not Modified

译者注：状态码为304的报文只有首部没有主体，因此很小。

Etags的问题是它们只能唯一的标识出某个站点的所有文件。如果浏览器尝试把来自某服务器文件的Etags传输给另一个服务器做校验，它不能起到任何作用，这种问题常常出现在使用了集群服务器的网站。

如果Etags没有匹配，那么用户将会收到一个200的状态码和所请求的文件。如果你使用Apache或IIS服务器集群，且使用默认的Etags配置，这将会带来性能问题。即使设置了`Expires`或`Last-Modified`字段，也会导致用户在刷新的时候需要重新下载缓存中的文件。

因此，可以考虑关闭`Etags`避免这种情况，这可以减少HTTP报文首部的大小。此外，在IIS8以上的版本，还可以简单的配置关闭Etags。

### 更早的刷新输出缓存

当用户请求页面时后端需要200~500ms来生成HTML文件。在这段时间内浏览器是空闲的直到请求数据抵达。例如PHP中可以使用`flush`函数将已生成的部分HTML发送给浏览器，这一般在后端负载较重而前端较轻的情况下较有用。

其中一个调用`flush`的好主意是在`<head>`标签之后，因为`<head>`标签容易生成且包含了CSS和JS等可以让浏览器在后端仍然繁忙的时候预先加载的一些文件。例如：

	<head>
		<!-- css, js go after this comment -->
	</head>
	<?php flush(); ?>
	<body>
		<!-- content goes after this -->
	</body>

### 使用AJAX的GET请求

Yahoo!邮件团队在使用`XMLHttpRequest`的时候发现，`POST`请求在浏览器中分为两步：发送请求首部，然后再发送主体。所以使用`GET`更好，因为它仅有需要一个TCP包就能发送所有数据（译者注：如果有大量的cookies信息就另当别论了）。在不同的浏览器中，URL长度限制不一样，但一般都会在2K或以上，这意味着你可以使用`GET`请求发送这么大的数据。

另一个有趣的地方是没有数据部分的`POST`请求会表现得与`GET`很类似。按[RFC-2616](http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html)所说，`GET`用于获取数据，因此使用`GET`来请求数据是合理的（也是语义化的），而不是用来向服务器发送数据。

### 避免空链接

具有空`src`属性的`<img>`会在这两种情况下出现：

不恰当的HTML：

	<img src="">

或者不恰当的JS：

	var img = new Image();
	// or var img = document.createElement("img");
	img.src = "";

这都会让浏览器发起向服务器的请求，在不同的浏览器下表现分别为：

* IE会请求当前页面所在的目录
* Chrome和Safari会请求当前页面
* FF3及之前版本和Chrome一样；但3.5之后的版本将不会发起请求
* Opera不会做任何事

这会导致：

* 增加服务器的请求数量，浪费带宽，加重负载。
* 浪费服务器资源生成无用的响应。
* 可能污染用户数据，特别是通过cookie和session追踪用户状态时。

译者注：除了img标签之外，像link、script、iframe等具有`src`或`href`属性的标签也会因空标签导致这个问题，需要谨慎对待。


## 分类标签：CSS

### 置顶CSS

当我们调查雅虎网页的性能时，发现把样式表放在HTML的`<head>`标签内会使得页面*感觉上*加载得更快。原因是这能让浏览器更早的渲染页面，也就是说，浏览器显示内容越快越好，特别是对于内容丰富的站点和网速较慢的用户来说。
译者注：有关link和@import，参见[先留个坑](#)

### 避免使用CSS表达式

CSS表达式可以很方便（也很危险）地动态设置CSS属性。从IE5开始支持该属性但从IE8开始被废弃。而且使用JS代码来控制网页行为改变样式效率高于直接使用CSS表达式。因此本文不翻译这一部分。

### 使用<link>而不是@import

之前一个关于CSS优化的建议是将其置顶，而在IE中使用`@import`相当于将`<link>`置底。因此不推荐。

译者注：`link`是HTML标签，`@import`是CSS关键字。关于这两个的区别以及优劣，可以参考[Don't use @import](http://www.stevesouders.com/blog/2009/04/09/dont-use-import/)一文，写的非常好。总之就是尽量不要用@import就对了2333.

### 避免使用filter（滤镜）

此处的filter指IE中的`AlphaImageLoader`，由于只在IE7及以下版本中使用，属于历史淘汰产物，此处不再赘述。

## 分类标签：JS

### 置底JS

HTTP1.1协议中规定浏览器对每个域名的并行连接数量不能超过两个（译者注：在之后的版本中废除了，但实际上现代浏览器依然存在这个限制，比如：Chrome最大并行限制是6个），因此加载大量的JS会阻塞其他的文件加载。

虽然可以通过把文件分散到多个域名下以增加并行连接的数量，但当浏览器解析`script`标签的时候，会阻塞其他文件的加载，即使它们在不同的域名。

另一个替代的解决方案是为`<script>`标签增加`defer=true`属性，但这种方法仅在IE系列浏览器中适用。（译者注：在HTML5中，截止本文写作时为止，defer属性已经被除Opera外的其他主流浏览器所支持。）

### 移除重复的JS

显然在页面上多次加载同一个JS会影响性能，但这个问题却并不像想象中的那么不常见。导致这种情况的两个主要原因是团队规模及JS数量。

以IE为例，如果一个外部脚本文件没有被缓存且被引入了两次，这会生成两个相同的HTTP请求。即使它已经被缓存了，在用户刷新页面的时候也会产生一个额外的请求。

解决的方法就是对页面引入依赖检查和增加文件版本号。比如PHP中的`insertScript`函数就能做到重复检查。

	<?php insertScript("menu.js") ?>

### 减少DOM操作

使用JS访问DOM元素开销不小，为了使页面响应速度更快，应该：

* 缓存访问过的DOM的引用
* “离线”更新节点，然后再插入DOM树
* 避免使用JS来布局

查看[http://yuiblog.com/blog/2007/12/20/video-lecomte/](http://yuiblog.com/blog/2007/12/20/video-lecomte/)可以了解更多这方面的细节。

译者注：“离线”更新节点，指使用`documentFragment`（文档碎片）或者不可见元素（`display:none`）作为容器，把所有需要更新的节点置于其中，然后一次性插入；或者直接更改innerHTML修改某个节点的子节点。这样的好处是不会触发浏览器多次重绘/重排。（不过现代浏览器使用了增量触发机制，即缓存更新直到某个程度才会触发重新渲染。）

同理，不恰当的使用JS修改样式或者直接使用JS布局也会多次触发浏览器的重绘/重排，导致页面卡顿，影响体验。

关于浏览器工作机理这方面的详细介绍，请参考[浏览器的工作原理](http://www.html5rocks.com/zh/tutorials/internals/howbrowserswork/)一文，比较长，但很有意思hhh。

### 使用智能事件处理器

当大量事件处理器被绑定到DOM树上且被频繁触发时，我们很可能会感觉页面卡顿。这可以使用`事件代理，Event Delegation`机制。比如`div`内有10个`button`，只需要将一个事件处理器绑定到`div`容器上，在事件的冒泡阶段可以捕获到事件，获悉哪一个`button`触发了该事件。

同样，不需要再等待DOM树的`onload`事件，而是当你所需要访问的所有元素就绪即可，比如不需要等待所有图像加载完成才开始运行JS脚本。可以用`DOMContentLoaded`来代替`onload`，但在所有浏览器都支持该事件之前（译者注：在翻译本文的时候，该事件已经得到除IE之外的主流浏览器支持），可以使用YUI Event库的`onAvailable`来代替。

## 分类标签：JS & CSS

### 使用外部JS和CSS

显然，JS和CSS文件都应该包含在外部文件中，而非内联在HTML里面。（译者注：从`内容-样式-表现`分离的开发规范来看，也应该这样做）

因为可以通过报文首部的设置让浏览器缓存它们，从而使得再次访问同样的页面能更快的打开，这适用于会被用户重复访问的页面以及可以重用的脚本（比如某些工具库和框架等）。

但是，有一种情况例外：比如yahoo!的首页这些某些用户只会访问一次的页面（译者注：这样自黑真的好吗），内联的JS和CSS可以减少请求次数从而达到更高效率。

### 精简JS和CSS

精简是从代码中除去不必要的字符达到减少大小，减少加载时间的目的，比如删除注释、空白字符等。像YUI Compressor就能压缩JS和CSS文件。

另外，变量名替换这样的方法也很有效。它比精简更为浮复杂，因此在简化后可能会产生bug。在US top10的站点中，精简能压缩21%的大小而替换则能做到25%。尽管后者压缩率更高，但前者更稳定可靠。（译者注：可以使用像Google Closure Compiler等工具做变量名替换简化代码，它还是比较靠谱的）。而且，即使在开启了Gzip压缩的情况下，精简代码仍然能压缩5%以上的大小。


## 分类标签：cookie

### 减少Cookie的大小

HTTP的cookie可以用于权限验证和身份信息等用途，而它位于服务器与浏览器之前交互的报文的首部之中。因此减少它的大小有利于减少加载时间。

更多的细节可以参考["When the Cookie Crumbles"](http://yuiblog.com/blog/2007/03/01/performance-research-part-3/)一文。这里列出一些：

* 减少不必要的cookies
* 减少cookie的大小以减少对用户加载时间的影响
* 需要对cookie设置合适的域名以保证子域名不受影响
* 合适的设置过期时间（Expires字段）。一个更长的期限或者不要过早移除cookie都能一定的改善用户体验。


### 为页面内容使用无cookie域名

当浏览器请求某些静态内容的时候会同时在首部中传输cookie，但服务器并不关心。因此，应该是用无cookie的报文请求静态文件，这需要创建一个子域名来存放静态内容。

假设你的域名是`www.example.org`，那你可以把静态内容存放在`static.example.org`。但假如为顶级域名`example.org`设置了cookie，那么即便是请求static域名下资源的时候也会在首部附带cookie信息。这时，你可以考虑购买另一个域名来管理静态资源。比如Yahoo!使用`yimg.com`，YouTube使用`ytimg.com`，Amazon使用`images-amazon.com`等。

另一个使用无cookie域名管理的好处，是一些代理服务器会拒绝缓存带有cookie请求的文件。因此，如果你希望使用`www.example.org`作为主页，那么需要考虑cookie的影响。输入域名的时候忽略了`www`，会导致cookie被设置到`*.example.org`（译者注：*是域名解析中的通配规则）。因此，好的做法应该是把cookie带有`www`的子域名上。

## 分类标签：图像

### 优化图像

在设计师的工作完成后，在把图片上传到服务器之前还需要做一些工作：

检查GIF使用的调色板规格是否与实际使用的颜色一致。使用[imagemagick](http://www.imagemagick.org/)可以轻松做到。

	identify -verbose image.gif

比如某张仅有4个颜色的GIF使用了256色的调色板，就说明它的大小还可以继续优化。

尝试把GIF转换成PNG看看能否节省空间。通常情况下是有的。开发者一般不愿意使用PNG因为浏览器兼容性不太好，但这是老皇历了。真正的问题是true PNG中的alpha透明通道问题。但由于GIF中没有真彩色也不支持alpha通道，因此GIF能做到的PNG也能做到。可以用这个命令简单的转换：

	convert image.gif image.png

对你的PNG文件运行[pngcrush](http://pmt.sourceforge.net/pngcrush/)检查是否还可以优化：

	pngcrush image.png -rem alla -reduce -brute result.png

对你的JPEG运行jpegtran，它可以对图片做一些无损压缩并且可以去掉图片中注释等无用信息（比如EXIF）：

	jpegtran -copy none -optimize -perfect src.jpg dest.jpg

### 优化CSS雪碧图（Sprites）

* 水平而不是垂直排列图像一般而言会得到更小的文件。
* 把相近的颜色放在雪碧图中可以减小颜色数量，理想的情况是小于256色以便使用PNG8的格式。
* 不要在雪碧图中留下太大的间隙，对移动端更友好。虽然间隙小不会显著减小雪碧图的尺寸，但是可以令客户端使用更少的内存把这些图像解析成位图。100x100有10,000像素，而1000x1000有1,000,000像素。

### 不要在HTML中缩放图像

不要因为可以使用HTML设置长度宽度就使用大图像。（译者注：其实应该是“用CSS缩放图像”才对）。如果你需要：

	<img width="100" height="100" src="pict.jpg" alt="Pict" />

的话，那么应该直接使用100*100的图片而不是把更大分辨率的图片缩小使用。（译者注：缩小图片可能会导致出现锯齿等情况影响质量）

### favicon要小且可缓存

`favicon.ico`储存在服务器根目录下，不管是否需要，浏览器都会请求它，因此最好不要返回`404 Not Found`。另外，当每次请求它的时候，cookie等信息会同时被包含在请求首部中。它同时会干扰你的下载队列。比如IE中，在`onload`中请求额外文件时，`favicon.ico`都会先于这些文件被加载。

尽可能减少favicon所带来不利影响的办法是：

* 把它的大小尽可能控制在1k以下
* 在合适的时候（不再更改它或者重命名）为它设置`Expires`首部字段，比如几个月的时间。并通过`Last-Modified`来让浏览器决定是否重新请求。
* [Imagemagick](http://www.imagemagick.org/)可以方便的创建favicon。

## 内容标签：移动端

### 把文件大小控制在25k以下

这个限制主要与iphone不能缓存大于25k的文件有关。注意这里指的是**未压缩的大小**，这就是需要精简的原因——因为仅仅Gzip还不够。

可以参考[Performance Research, Part 5: iPhone Cacheability - Making it Stick](http://yuiblog.com/blog/2008/02/06/iphone-cacheability/)了解更多。

### 把组件打包成复合文档

把组件打包成复合文档就像带有多个附件的E-mail。因为HTTP请求开销很大，但是在使用该方法前，要先检查浏览器是否支持。

译者注：详情参考[MHTML](https://en.wikipedia.org/wiki/MHTML)。但本人认为，控制好外部JS和CSS的缓存能更好的提升性能，同时也比这个方法更具有实际意义。











·